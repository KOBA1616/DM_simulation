
import sys
import os
import unittest
import torch
import numpy as np
from pathlib import Path

# Add project root to path
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

try:
    import dm_ai_module
    from dm_toolkit.ai.agent.transformer_model import DuelTransformer
except ImportError as e:
    print(f"ImportError: {e}")
    sys.exit(1)

class TestInferenceIntegration(unittest.TestCase):
    def setUp(self):
        self.game = dm_ai_module.GameInstance()
        self.game.start_game()

        # Ensure we have a dummy model file or mock it.
        # Ideally, we load the one generated by train_simple.py
        # But if it doesn't exist, we create a fresh one for this test.
        self.model_path = Path("models/duel_transformer_latest.pth")

        self.input_dim = 100 # Should match training/simple_game_generator.py
        self.action_dim = 20 # Should match training/simple_game_generator.py
        self.model = DuelTransformer(self.input_dim, self.action_dim)

        if not self.model_path.exists():
            print("[Test] Model not found, creating a dummy one.")
            torch.save(self.model.state_dict(), self.model_path)

    def test_load_and_infer(self):
        """
        Verify Issue 3: Inference Integration.
        1. Load Model from .pth
        2. Encode Game State
        3. Run Inference
        4. Check output validity
        """
        # 1. Load Model
        print(f"\n[Test] Loading model from {self.model_path}")
        try:
            checkpoint = torch.load(self.model_path)
            self.model.load_state_dict(checkpoint)
            self.model.eval()
        except Exception as e:
            self.fail(f"Failed to load model: {e}")

        # 2. Encode Game State
        # In a real scenario, we use TokenConverter.
        # For this test, we mock the input vector matching input_dim.
        # DuelTransformer expects LongTensor for embedding layer if it takes indices.
        # Checking dm_toolkit/ai/agent/transformer_model.py logic, it uses Embedding.
        # So we must pass LongTensor (integer indices), not FloatTensor.

        # We need to pass indices within the vocab range (0 to input_dim-1 usually, or larger).
        # Assuming input_dim is the vocab size or similar.
        # Let's check `transformer_model.py` source or just assume input_dim is num_embeddings.
        # If input_dim=100 passed to constructor, it likely means vocab_size=100.

        # Input shape should be (Batch, Seq_Len).
        # The model constructor is DuelTransformer(input_dim, action_dim).
        # If input_dim is passed as `num_tokens`, then we pass indices < input_dim.

        seq_len = 64 # Arbitrary sequence length that the model might expect or handle
        # Or maybe it just takes a fixed size vector.
        # Most transformers take (B, T).

        state_tensor = torch.randint(0, self.input_dim, (1, seq_len), dtype=torch.long)

        # 3. Run Inference
        print("[Test] Running inference...")
        with torch.no_grad():
            policy_logits, value = self.model(state_tensor)

        # 4. Check Output
        print(f"[Test] Policy Output Shape: {policy_logits.shape}")
        print(f"[Test] Value Output: {value.item()}")

        self.assertEqual(policy_logits.shape, (1, self.action_dim))
        self.assertEqual(value.shape, (1, 1))

        # Check if we can get a valid action index
        probs = torch.softmax(policy_logits, dim=-1)
        action_idx = torch.argmax(probs).item()
        print(f"[Test] Selected Action Index: {action_idx}")

        self.assertTrue(0 <= action_idx < self.action_dim)

if __name__ == '__main__':
    unittest.main()
