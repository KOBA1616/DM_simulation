# 非公開領域推論システム設計書
## ～ POMDP (部分観測マルコフ決定過程) への対応戦略 ～

## 1. 概要
本システムは、TCGにおける「見えていない情報（相手の手札、自分の山札の残り、シールドの中身）」を、**「16次元スタッツ」**を用いて数値的に推論するための拡張モジュールである。

これにより、AIは単なる「確率の高い手を打つ機械」から、以下のような高度な判断を行う**「勝負師」**へと進化する。
*   **リスク管理**: 「山札にトリガーがないから、絶対にブロックする」
*   **確率操作**: 「キーカードを引く確率を上げるために、不要なカードをマナに埋める（山札圧縮）」
*   **読み**: 「相手はマナがあるのに動かなかった。除去カードを温存しているはずだ」

## 2. システム構成：2つの推論スコープ
推論対象を「自分（確定確率）」と「相手（不確定推論）」に分割し、それぞれ最適な手法で実装する。

| 対象 | 推論内容 | 実装アプローチ | 計算コスト |
| :--- | :--- | :--- | :--- |
| **Self (自分)** | 山札・シールドの期待値 | 差分更新法 (O(1)) | 極小 |
| **Opponent (相手)** | 手札・山札の傾向予測 | 知識の蒸留 (Teacher-Student) | 学習時: 高 / 推論時: 低 |

## 3. 実装詳細：自分自身の推論 (Self-Inference)
自分のデッキの中身は既知であるため、「初期デッキ」から「見えているカード」を引くことで、残りの領域の成分を正確に算出できる。

### 3.1 入力ベクトルの拡張
AIへの入力（Observation）に以下の32次元を追加する。

#### A. Library Potential (山札ポテンシャル / 16次元)
*   **定義**: 山札に残っているカードのスタッツ平均値。
*   **AIの思考**: 「次のドローで『解決札（Removal）』を引ける確率は？」
*   **計算式**: `(Initial_Deck_Sum - Visible_Sum) / Remaining_Count`

#### B. Shield Expectation (シールド期待値 / 16次元)
*   **定義**: シールドゾーンに残っているカードのスタッツ平均値。
*   **AIの思考**: 「殴らせてトリガー（Trigger Rate）が出る確率は？」
*   **計算式**: 基本的に山札と同じ確率分布だが、サーチやシールド化効果で操作された場合は個別に追跡する。

### 3.2 C++ エンジン実装ロジック
毎回スキャンするのではなく、**「全体から引く」**ことで高速化する。

```cpp
class GameState {
private:
    // ゲーム開始時のデッキ全体のスタッツ合計（固定値）
    CardStats initial_deck_sum;
    
    // 公開領域にあるカードのスタッツ合計（変動値）
    // (手札 + マナ + 墓地 + 盤面 + 表向きシールド)
    CardStats visible_sum;

    int initial_deck_count = 40;
    int current_visible_count = 0;

public:
    // カードが公開領域に移動した時に呼ぶ（ドロー、マナチャージ、プレイ等）
    void on_card_reveal(int card_id) {
        CardStats stats = get_card_stats(card_id);
        visible_sum += stats; // 加算演算子オーバーロード
        current_visible_count++;
    }

    // AI入力用：山札の期待値ベクトル取得
    std::vector<float> get_library_potential() {
        int remaining = initial_deck_count - current_visible_count;
        if (remaining <= 0) return std::vector<float>(16, 0.0f);

        // 残存合計 = 全体 - 見えている分
        CardStats remaining_sum = initial_deck_sum - visible_sum;
        
        // 平均化 (Normalize)
        return remaining_sum.divide_by(remaining);
    }
};
```

## 4. 実装詳細：対戦相手の推論 (Opponent Inference)
相手の手札は見えないため、計算ではなく**「学習（直感）」によって推論する。**
本番では見えないが、学習中のみ「カンニング」を行う**「先生と生徒（Teacher-Student）」**方式を採用する。

### 4.1 学習アーキテクチャ
*   **先生モデル (Teacher Model)**
    *   **権限**: 神の視点 (God View)。相手の手札、シールドの中身が全て見えている。
    *   **入力**: `[自分の情報] + [相手の公開情報] + [相手の非公開情報(手札・盾)]`
    *   **役割**: 「ここは殴るとトリガーを踏むから待機だ」という正解のプレイングを導き出す。
*   **生徒モデル (Student Model / 本番用)**
    *   **権限**: 通常視点。非公開情報は全て 0 または マスク されている。
    *   **入力**: `[自分の情報] + [相手の公開情報]`
    *   **役割**: 先生の動きを模倣することで、間接的に「読み」を習得する。

### 4.2 蒸留 (Distillation) トレーニングフロー
Python (PyTorch) 側での学習ループ。

```python
def train_step(student_net, teacher_net, env, optimizer):
    # 1. 環境から情報を取得
    # obs: 通常の観測情報
    # hidden_info: 相手の手札・シールド情報 (学習時のみ取得可能)
    obs, hidden_info = env.reset()
    
    # 2. 先生の「正解」を取得 (勾配計算不要)
    with torch.no_grad():
        # 先生は全てが見えている状態で判断する
        teacher_full_input = torch.cat([obs, hidden_info], dim=1)
        teacher_action_logits = teacher_net(teacher_full_input)
    
    # 3. 生徒の推論
    # 生徒は公開情報だけで判断する
    student_action_logits = student_net(obs)
    
    # 4. 損失計算 (KLダイバージェンス)
    # 「勝つこと(Reward)」だけでなく「先生と同じ思考(Logits)になること」を目指す
    loss_distill = KL_div(student_action_logits, teacher_action_logits)
    loss_reward = PPO_loss(...) 
    
    total_loss = loss_distill + loss_reward
    
    total_loss.backward()
    optimizer.step()
```

### 4.3 推論される内容 (AIの獲得スキル)
この学習を経ることで、生徒モデルは以下の「メタ読み」を獲得する。
*   **ハンド・リーディング (手札読み)**:
    *   **現象**: 先生は「相手がマナを残してターン終了した時」に攻撃を控えた。
    *   **学習**: 生徒は「相手のマナが立っている＝カウンターの危険性が高い」という相関関係（重み）を獲得する。
*   **デッキ・プロファイリング (山札読み)**:
    *   **現象**: 先生は「相手のマナに『速攻カード』が多い時」に、シールドを殴らず盤面処理を優先した。
    *   **学習**: 生徒は「見えているカードが速攻系なら、見えていない手札も速攻系（除去耐性なし）だろう」と推論するようになる。

## 5. データ構造：16次元スタッツ (再掲)
推論のベースとなる共通言語。相手の手札推論も、自分の山札確率も、全てこの16次元ベクトルで行われる。

| No | 次元名 | AIの解釈例 |
| :--- | :--- | :--- |
| 0-3 | **Timing & Cost** | 初動か？ トリガーか？ 踏み倒しか？ |
| 4-7 | **Advantage** | ドロー、除去、ブースト、ブレイク性能 |
| 8-11 | **Risk & Stability** | ランダム性、場持ち、効果破壊耐性 |
| 12-15 | **Impact** | 勝率貢献、逆転力、リーサル性能、LO適正 |

### 5.1 AIへの入力ベクトル補正（デッキ判定時）
デッキタイプの判定時、単に「コスト」の平均をとるのではなく、**「軽減持ちカードは、仮想コストで平均をとる」**計算を行う。

*   **例**: コスト8、G・ゼロ持ちのクリーチャー
    *   通常の平均コスト計算 → 8 として計算（重いデッキと判定される＝間違い）
    *   補正後の計算 → 2 ～ 3 程度（期待値）として計算（速攻デッキと判定される＝正解）

### 5.2 動的データ： 「現在コスト (Current Cost)」の入力
対戦中（強化学習中）、AIへの入力特徴量（`vectorize_state`）には、カードに書かれているコストではなく、**「今の盤面で実際に支払うコスト」**を入力する。

## 6. まとめ：導入ステップ
1.  **Step 1 (Self)**: C++エンジンに `CardStats` の差分更新ロジックを実装し、自分の山札・シールドの平均スタッツを出力できるようにする。
2.  **Step 2 (Teacher)**: Python側で、相手の手札情報を入力に含めた「チートモデル」を学習させ、最強の教師を作成する。
3.  **Step 3 (Distillation)**: 教師の思考を通常モデル（生徒）に蒸留させ、「見えないものを見る力」を実装する。

これにより、運否天賦ではなく**「確率と読み」**で戦う最強のAIが完成する。
