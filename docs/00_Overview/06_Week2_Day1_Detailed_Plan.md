# Phase 4 Week 2 Day 1 å®Ÿè£…è¨ˆç”»ï¼ˆ1æœˆ13æ—¥ï¼‰

**å‰ææ¡ä»¶**:
- âœ… Q1: SynergyåˆæœŸåŒ– = æ‰‹å‹•å®šç¾©ã§é–‹å§‹
- âœ… Q2: CLSãƒˆãƒ¼ã‚¯ãƒ³ä½ç½® = å…ˆé ­ï¼ˆ[CLS] tokenï¼‰
- âœ… Q3: ãƒãƒƒãƒã‚µã‚¤ã‚º = 8â†’16â†’32â†’64 æ®µéšçš„æ‹¡å¤§
- âœ… Q4: **ãƒ‡ãƒ¼ã‚¿ç¾æ³ = ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ãªã—â†’æ–°è¦ç”Ÿæˆå¿…é ˆ**
- â³ Q5-Q9: å®Ÿè£…ä¸­ã«æ±ºå®šå¯èƒ½ï¼ˆæ¨å¥¨å€¤ã‚ã‚Šï¼‰

**ä½œæ¥­æ™‚é–“é…åˆ†**: 8æ™‚é–“

---

## Task 1: Synergy ãƒãƒˆãƒªãƒƒã‚¯ã‚¹ï¼ˆæ‰‹å‹•å®šç¾©ï¼‰å®Ÿè£…

**æ‰€è¦æ™‚é–“**: 2.5æ™‚é–“  
**æ‹…å½“**: TransformeråˆæœŸåŒ–ãƒ•ã‚§ãƒ¼ã‚º

### 1.1 æ‰‹å‹•å®šç¾©ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ

**ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ**: [data/synergy_pairs_v1.json](../../data/synergy_pairs_v1.json)

```json
{
  "description": "Manual Synergy Pairs (Phase 4 v1)",
  "version": "1.0",
  "pairs": [
    {
      "name": "Revolution Change with Multi-Color",
      "card_ids": [101, 205],
      "synergy_score": 0.8,
      "description": "å¤šè‰²ã‚«ãƒ¼ãƒ‰ã‚’è¸ã¿å°ã«ã—ã¦é©å‘½ãƒã‚§ãƒ³ã‚¸ã™ã‚‹å¼·åŠ›ãªã‚³ãƒ³ãƒœ"
    },
    {
      "name": "Shield Trigger Chain",
      "card_ids": [150, 151],
      "synergy_score": 0.7,
      "description": "ã‚·ãƒ¼ãƒ«ãƒ‰ãƒˆãƒªã‚¬ãƒ¼ã®é€£é–åŠ¹æœ"
    },
    {
      "name": "Mana Ramp Combo",
      "card_ids": [50, 51, 52],
      "synergy_score": 0.6,
      "description": "ãƒãƒŠåŠ é€Ÿã‚³ãƒ³ãƒœï¼ˆè¤‡æ•°ã‚«ãƒ¼ãƒ‰ã®ç›¸ä¹—åŠ¹æœï¼‰"
    },
    {
      "name": "Creature Synergy - Evolution",
      "card_ids": [200, 201, 202],
      "synergy_score": 0.75,
      "description": "é€²åŒ–ã‚¯ãƒªãƒ¼ãƒãƒ£ãƒ¼ã®é€²åŒ–ãƒã‚§ãƒ¼ãƒ³"
    }
  ],
  "notes": "ã‚«ãƒ¼ãƒ‰IDã¯ TOKEN_CARD_OFFSET=100 ã‚’åŸºæº–ã«è¨­å®š"
}
```

### 1.2 SynergyGraph ã¸ã®æ‰‹å‹•å®šç¾©ãƒ­ãƒ¼ãƒ‰æ©Ÿèƒ½è¿½åŠ 

**ãƒ•ã‚¡ã‚¤ãƒ«ä¿®æ­£**: [dm_toolkit/ai/agent/synergy.py](../../dm_toolkit/ai/agent/synergy.py)

```python
# è¿½åŠ ãƒ¡ã‚½ãƒƒãƒ‰

@classmethod
def from_manual_pairs(
    cls,
    vocab_size: int,
    pairs_json_path: str,
    embedding_dim: int = 64
) -> 'SynergyGraph':
    """
    æ‰‹å‹•å®šç¾©ãƒšã‚¢ã‹ã‚‰ SynergyGraph ã‚’åˆæœŸåŒ–ã€‚
    
    Args:
        vocab_size: ãƒˆãƒ¼ã‚¯ãƒ³èªå½™ã‚µã‚¤ã‚ºï¼ˆ1000ï¼‰
        pairs_json_path: ã‚«ãƒ¼ãƒ‰ç›¸æ€§ãƒšã‚¢ JSON ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        embedding_dim: ã‚·ãƒŠã‚¸ãƒ¼åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒï¼ˆ64ï¼‰
    
    Returns:
        SynergyGraph ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
    """
    import json
    
    instance = cls(vocab_size, embedding_dim)
    
    # JSON ã‹ã‚‰ç›¸æ€§ãƒšã‚¢ã‚’èª­ã¿è¾¼ã¿
    with open(pairs_json_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    # å›ºå®šã‚¹ã‚³ã‚¢è¡Œåˆ—ã‚’åˆæœŸåŒ–
    synergy_matrix = torch.zeros(vocab_size, vocab_size)
    
    for pair_info in data['pairs']:
        card_ids = pair_info['card_ids']
        score = pair_info['synergy_score']
        
        # Symmetric ãªç›¸æ€§ã‚¹ã‚³ã‚¢ã‚’è¨­å®š
        for i in card_ids:
            for j in card_ids:
                if i != j:
                    synergy_matrix[i, j] = score
    
    # å›ºå®šè¡Œåˆ—ã‚’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨ã—ã¦ä¿å­˜ï¼ˆrequires_grad=Falseï¼‰
    instance.fixed_synergy_matrix = torch.nn.Parameter(
        synergy_matrix,
        requires_grad=False
    )
    instance.use_fixed_matrix = True
    
    return instance

def get_bias_for_sequence(self, sequence: torch.Tensor) -> torch.Tensor:
    """
    æ”¹è‰¯ç‰ˆ: å›ºå®šè¡Œåˆ—ã¨å­¦ç¿’å¯èƒ½åŸ‹ã‚è¾¼ã¿ã®ä¸¡æ–¹ã‚’ã‚µãƒãƒ¼ãƒˆ
    """
    B, S = sequence.shape
    
    if hasattr(self, 'use_fixed_matrix') and self.use_fixed_matrix:
        # å›ºå®šè¡Œåˆ—ã‚’ä½¿ç”¨
        # sequence[b, s] ã®å€¤ã‚’ä½¿ç”¨ã—ã¦è¡Œåˆ—ã‹ã‚‰å€¤ã‚’å‚ç…§
        bias = torch.zeros(B, S, S, device=sequence.device)
        for b in range(B):
            for i in range(S):
                for j in range(S):
                    card_i = sequence[b, i].item()
                    card_j = sequence[b, j].item()
                    if card_i < self.fixed_synergy_matrix.shape[0] and \
                       card_j < self.fixed_synergy_matrix.shape[1]:
                        bias[b, i, j] = self.fixed_synergy_matrix[card_i, card_j]
        return bias
    else:
        # å…ƒã®å®Ÿè£…: åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã®å†…ç©
        embs = cast(torch.Tensor, self.synergy_embeddings(sequence))
        bias = torch.bmm(embs, embs.transpose(1, 2))
        bias = bias / (self.embedding_dim ** 0.5)
        return bias
```

### 1.3 å˜ä½“ãƒ†ã‚¹ãƒˆå®Ÿè£…

**ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ**: [tests/test_synergy_manual.py](../../tests/test_synergy_manual.py)

```python
import pytest
import torch
import json
import tempfile
import os
from dm_toolkit.ai.agent.synergy import SynergyGraph

def test_synergy_from_manual_pairs():
    """Test loading synergy from manual pairs JSON."""
    
    # ãƒ†ã‚¹ãƒˆç”¨ JSON ã‚’ä¸€æ™‚ä½œæˆ
    pairs_data = {
        "pairs": [
            {"card_ids": [100, 101], "synergy_score": 0.8},
            {"card_ids": [200, 201], "synergy_score": 0.6},
        ]
    }
    
    with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:
        json.dump(pairs_data, f)
        temp_path = f.name
    
    try:
        # SynergyGraph ã‚’ä½œæˆ
        synergy = SynergyGraph.from_manual_pairs(
            vocab_size=1000,
            pairs_json_path=temp_path,
            embedding_dim=64
        )
        
        # å›ºå®šè¡Œåˆ—ãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª
        assert hasattr(synergy, 'fixed_synergy_matrix')
        assert synergy.use_fixed_matrix
        
        # ã‚¹ã‚³ã‚¢ãŒæ­£ç¢ºã«è¨­å®šã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª
        assert synergy.fixed_synergy_matrix[100, 101].item() == 0.8
        assert synergy.fixed_synergy_matrix[200, 201].item() == 0.6
        
        # Symmetric æ€§ç¢ºèªï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        assert synergy.fixed_synergy_matrix[101, 100].item() == 0.8
        
        # get_bias_for_sequence() ã§å€¤ãŒå–å¾—ã§ãã‚‹ã‹ç¢ºèª
        tokens = torch.tensor([[100, 101, 0], [200, 201, 0]])  # [batch=2, seq=3]
        bias = synergy.get_bias_for_sequence(tokens)
        
        assert bias.shape == (2, 3, 3)
        assert bias[0, 0, 1].item() == 0.8  # card 100-101 ç›¸æ€§
        
        print("âœ… test_synergy_from_manual_pairs passed")
        
    finally:
        os.unlink(temp_path)

if __name__ == "__main__":
    test_synergy_from_manual_pairs()
```

**å®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰**:
```bash
pytest tests/test_synergy_manual.py -v
```

**ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ**:
- [ ] synergy_pairs_v1.json ä½œæˆ
- [ ] SynergyGraph.from_manual_pairs() å®Ÿè£…
- [ ] get_bias_for_sequence() æ”¹è‰¯
- [ ] test_synergy_manual.py âœ… å®Ÿè¡ŒæˆåŠŸ

---

## Task 2: ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

**æ‰€è¦æ™‚é–“**: 3.0æ™‚é–“  
**æ‹…å½“**: ãƒ‡ãƒ¼ã‚¿æº–å‚™ãƒ•ã‚§ãƒ¼ã‚º

### 2.1 ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆã‚¹ã‚¯ãƒªãƒ—ãƒˆä½œæˆ

**ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ**: [generate_transformer_training_data.py](../../generate_transformer_training_data.py)

```python
#!/usr/bin/env python3
"""
Generate Transformer training data from self-play scenarios.

Output format:
    - tokens: [num_samples, seq_len] int64 token IDs
    - policies: [num_samples, action_dim] float32 policy targets
    - values: [num_samples, 1] float32 value targets
"""

import os
import sys
import argparse
import numpy as np
import torch
from typing import List, Tuple
from tqdm import tqdm

# Setup paths
project_root = os.path.abspath(os.path.dirname(__file__))
sys.path.insert(0, project_root)

# Dynamic import
dm_ai_module = None
try:
    import dm_ai_module
except ImportError:
    print("âš ï¸  Could not import dm_ai_module. Token generation will be mocked.")

from dm_toolkit.ai.agent.transformer_model import DuelTransformer
from dm_toolkit.ai.agent.synergy import SynergyGraph
from dm_toolkit.training.scenario_runner import ScenarioRunner

def generate_samples(
    num_samples: int = 1000,
    output_path: str = "data/training_data.npz",
    vocab_size: int = 1000,
    max_seq_len: int = 200
) -> None:
    """Generate Transformer training data from scenarios."""
    
    print(f"Generating {num_samples} training samples...")
    
    all_tokens = []
    all_policies = []
    all_values = []
    
    # Scenario data ã‚’èª­ã¿è¾¼ã¿
    runner = ScenarioRunner(scenario_names=['basic', 'advanced'])
    
    for sample_idx in tqdm(range(num_samples)):
        try:
            # ã‚²ãƒ¼ãƒ  1 è©¦è¡Œã‚’å®Ÿè¡Œ
            game_data = runner.run_scenario()
            
            # GameState â†’ Tokensï¼ˆC++ TensorConverter ä½¿ç”¨ï¼‰
            if dm_ai_module:
                tokens = dm_ai_module.convert_to_sequence(
                    game_data['state'],
                    player_view=0,
                    mask_opponent_hand=True
                )
            else:
                # Fallback: ãƒ©ãƒ³ãƒ€ãƒ ãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆãƒ†ã‚¹ãƒˆç”¨ï¼‰
                tokens = np.random.randint(
                    0, vocab_size,
                    size=np.random.randint(50, max_seq_len)
                )
            
            # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
            if len(tokens) < max_seq_len:
                tokens = np.pad(
                    tokens,
                    (0, max_seq_len - len(tokens)),
                    constant_values=0
                )
            else:
                tokens = tokens[:max_seq_len]
            
            all_tokens.append(tokens)
            
            # Policy & Value targets
            policy_target = game_data['policy']  # [action_dim]
            value_target = game_data['value']     # scalar
            
            all_policies.append(policy_target)
            all_values.append([value_target])
        
        except Exception as e:
            print(f"  âš ï¸  Sample {sample_idx} generation failed: {e}")
            continue
    
    # NPZ ã§ä¿å­˜
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    
    tokens_array = np.array(all_tokens, dtype=np.int64)
    policies_array = np.array(all_policies, dtype=np.float32)
    values_array = np.array(all_values, dtype=np.float32)
    
    np.savez(
        output_path,
        tokens=tokens_array,
        policies=policies_array,
        values=values_array
    )
    
    print(f"\nâœ… Generated {len(all_tokens)} samples")
    print(f"   Tokens shape: {tokens_array.shape}")
    print(f"   Policies shape: {policies_array.shape}")
    print(f"   Values shape: {values_array.shape}")
    print(f"   Saved to: {output_path}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Generate Transformer training data")
    parser.add_argument("--num-samples", type=int, default=1000, help="Number of samples")
    parser.add_argument("--output", type=str, default="data/training_data.npz", help="Output path")
    parser.add_argument("--vocab-size", type=int, default=1000, help="Vocabulary size")
    parser.add_argument("--max-seq-len", type=int, default=200, help="Max sequence length")
    
    args = parser.parse_args()
    
    generate_samples(
        num_samples=args.num_samples,
        output_path=args.output,
        vocab_size=args.vocab_size,
        max_seq_len=args.max_seq_len
    )
```

**å®Ÿè¡Œä¾‹**:
```bash
python generate_transformer_training_data.py --num-samples 1000 --output data/training_data.npz
```

### 2.2 ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰æ¤œè¨¼ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

**ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ**: [tests/test_training_data_load.py](../../tests/test_training_data_load.py)

```python
import pytest
import numpy as np
import torch
from torch.utils.data import DataLoader
from dm_toolkit.training.training_pipeline import DuelDataset, collate_batch

def test_training_data_load_and_batch():
    """Test loading generated training data and batching."""
    
    # ãƒ†ã‚¹ãƒˆç”¨ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
    num_samples = 10
    max_seq_len = 200
    action_dim = 100
    
    tokens = np.random.randint(0, 1000, size=(num_samples, max_seq_len), dtype=np.int64)
    policies = np.random.randn(num_samples, action_dim).astype(np.float32)
    values = np.random.randn(num_samples, 1).astype(np.float32)
    
    # Dataset ä½œæˆ
    tokens_list = [torch.from_numpy(tokens[i]) for i in range(num_samples)]
    dataset = DuelDataset(
        tokens=tokens_list,
        states=None,
        policies=torch.from_numpy(policies),
        values=torch.from_numpy(values)
    )
    
    # DataLoader ã§ãƒãƒƒãƒå‡¦ç†
    loader = DataLoader(
        dataset,
        batch_size=4,
        collate_fn=collate_batch
    )
    
    # ãƒãƒƒãƒã‚’ç¢ºèª
    for batch_idx, batch in enumerate(loader):
        assert 'tokens' in batch
        assert 'padding_mask' in batch
        assert 'policy' in batch
        assert 'value' in batch
        
        tokens_batch = batch['tokens']
        padding_mask = batch['padding_mask']
        
        assert tokens_batch.shape == (4, max_seq_len)
        assert padding_mask.shape == (4, max_seq_len)
        assert tokens_batch.dtype == torch.int64
        assert padding_mask.dtype == torch.bool
        
        print(f"âœ… Batch {batch_idx}: tokens {tokens_batch.shape}, mask {padding_mask.shape}")
    
    print("âœ… test_training_data_load_and_batch passed")

if __name__ == "__main__":
    test_training_data_load_and_batch()
```

**ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ**:
- [ ] generate_transformer_training_data.py å®Ÿè£…
- [ ] æœ€åˆã® 100 ã‚µãƒ³ãƒ—ãƒ«ç”Ÿæˆãƒ»ç¢ºèª
- [ ] test_training_data_load.py âœ… å®Ÿè¡ŒæˆåŠŸ
- [ ] tokens shape ãŒ [N, 200] ã§çµ±ä¸€ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª

---

## Task 3: Transformer å­¦ç¿’ã‚¹ã‚¯ãƒªãƒ—ãƒˆå®Ÿè£…

**æ‰€è¦æ™‚é–“**: 2.5æ™‚é–“  
**æ‹…å½“**: ãƒ¢ãƒ‡ãƒ«çµ±åˆãƒ†ã‚¹ãƒˆãƒ•ã‚§ãƒ¼ã‚º

### 3.1 åŸºæœ¬å­¦ç¿’ãƒ«ãƒ¼ãƒ—å®Ÿè£…

**ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ**: [train_transformer_phase4.py](../../train_transformer_phase4.py)

```python
#!/usr/bin/env python3
"""
Phase 4 Transformer Training Script

Week 2 Goal: Verify model initialization and basic training loop
"""

import os
import sys
import argparse
import numpy as np
import torch
import torch.nn.functional as F
import torch.optim as optim
from pathlib import Path
from datetime import datetime
from torch.utils.data import DataLoader

project_root = os.path.abspath(os.path.dirname(__file__))
sys.path.insert(0, project_root)

from dm_toolkit.ai.agent.transformer_model import DuelTransformer
from dm_toolkit.ai.agent.synergy import SynergyGraph
from dm_toolkit.training.training_pipeline import DuelDataset, collate_batch

class TransformerTrainer:
    def __init__(
        self,
        vocab_size: int = 1000,
        action_dim: int = 100,
        d_model: int = 256,
        nhead: int = 8,
        num_layers: int = 6,
        max_len: int = 200,
        synergy_pairs_path: str = "data/synergy_pairs_v1.json",
        device: str = "cuda" if torch.cuda.is_available() else "cpu"
    ):
        self.device = device
        self.vocab_size = vocab_size
        self.action_dim = action_dim
        
        print(f"ğŸ”§ Initializing TransformerTrainer on {device.upper()}")
        
        # 1. Synergy Graph ãƒ­ãƒ¼ãƒ‰
        try:
            self.synergy_graph = SynergyGraph.from_manual_pairs(
                vocab_size=vocab_size,
                pairs_json_path=synergy_pairs_path
            )
            print(f"âœ… Synergy pairs loaded from {synergy_pairs_path}")
        except Exception as e:
            print(f"âš ï¸  Could not load synergy pairs: {e}")
            print("   Using default (random) synergy initialization")
            self.synergy_graph = SynergyGraph(vocab_size=vocab_size)
        
        # 2. Model åˆæœŸåŒ–
        self.model = DuelTransformer(
            vocab_size=vocab_size,
            action_dim=action_dim,
            d_model=d_model,
            nhead=nhead,
            num_layers=num_layers,
            max_len=max_len,
            synergy_matrix_path=None  # SynergyGraph ã§ç®¡ç†
        ).to(device)
        
        # Synergy Graph ã‚’ model ã«çµåˆï¼ˆå…±æœ‰ï¼‰
        self.model.synergy_graph = self.synergy_graph
        
        print(f"âœ… Model initialized")
        print(f"   - Parameters: {sum(p.numel() for p in self.model.parameters()):,}")
        print(f"   - Device: {device}")
        
        # 3. Optimizer
        self.optimizer = optim.Adam(
            self.model.parameters(),
            lr=1e-4,
            weight_decay=1e-5
        )
        
        # 4. Learning rate scheduler
        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer,
            T_max=100,
            eta_min=1e-6
        )
        
        # Metrics
        self.metrics = {
            'train_policy_loss': [],
            'train_value_loss': [],
            'train_total_loss': [],
            'val_policy_loss': [],
            'val_value_loss': []
        }
    
    def train_epoch(self, loader: DataLoader, epoch: int) -> dict:
        """Train for one epoch."""
        self.model.train()
        
        policy_losses = []
        value_losses = []
        total_losses = []
        
        for batch_idx, batch in enumerate(loader):
            tokens = batch['tokens'].to(self.device)
            padding_mask = batch['padding_mask'].to(self.device)
            policy_target = batch['policy'].to(self.device)
            value_target = batch['value'].to(self.device)
            
            # Forward pass
            policy_logits, value_pred = self.model(tokens, padding_mask)
            
            # Loss computation
            policy_loss = F.cross_entropy(
                policy_logits,
                policy_target.argmax(dim=1)
            )
            value_loss = F.mse_loss(value_pred, value_target)
            total_loss = policy_loss + value_loss
            
            # Backward
            self.optimizer.zero_grad()
            total_loss.backward()
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
            self.optimizer.step()
            
            # Metrics
            policy_losses.append(policy_loss.item())
            value_losses.append(value_loss.item())
            total_losses.append(total_loss.item())
            
            if (batch_idx + 1) % 10 == 0:
                print(f"  Epoch {epoch} [{batch_idx+1}] "
                      f"Policy Loss: {policy_loss.item():.4f}, "
                      f"Value Loss: {value_loss.item():.4f}")
        
        return {
            'policy_loss': np.mean(policy_losses),
            'value_loss': np.mean(value_losses),
            'total_loss': np.mean(total_losses)
        }
    
    @torch.no_grad()
    def validate(self, loader: DataLoader) -> dict:
        """Validate on a dataset."""
        self.model.eval()
        
        policy_losses = []
        value_losses = []
        
        for batch in loader:
            tokens = batch['tokens'].to(self.device)
            padding_mask = batch['padding_mask'].to(self.device)
            policy_target = batch['policy'].to(self.device)
            value_target = batch['value'].to(self.device)
            
            policy_logits, value_pred = self.model(tokens, padding_mask)
            
            policy_loss = F.cross_entropy(
                policy_logits,
                policy_target.argmax(dim=1)
            )
            value_loss = F.mse_loss(value_pred, value_target)
            
            policy_losses.append(policy_loss.item())
            value_losses.append(value_loss.item())
        
        return {
            'policy_loss': np.mean(policy_losses),
            'value_loss': np.mean(value_losses)
        }
    
    def train(
        self,
        train_loader: DataLoader,
        val_loader: DataLoader = None,
        epochs: int = 10,
        checkpoint_dir: str = "checkpoints/phase4"
    ) -> None:
        """Full training loop."""
        os.makedirs(checkpoint_dir, exist_ok=True)
        
        print(f"\nğŸš€ Starting training: {epochs} epochs")
        print(f"   Checkpoint dir: {checkpoint_dir}\n")
        
        best_val_loss = float('inf')
        
        for epoch in range(epochs):
            print(f"Epoch {epoch+1}/{epochs}")
            
            # Train
            train_metrics = self.train_epoch(train_loader, epoch)
            print(f"  Train - Policy Loss: {train_metrics['policy_loss']:.4f}, "
                  f"Value Loss: {train_metrics['value_loss']:.4f}")
            
            # Validation
            if val_loader:
                val_metrics = self.validate(val_loader)
                total_val_loss = val_metrics['policy_loss'] + val_metrics['value_loss']
                print(f"  Val   - Policy Loss: {val_metrics['policy_loss']:.4f}, "
                      f"Value Loss: {val_metrics['value_loss']:.4f}")
                
                # Save best checkpoint
                if total_val_loss < best_val_loss:
                    best_val_loss = total_val_loss
                    self._save_checkpoint(
                        epoch,
                        checkpoint_dir,
                        train_metrics,
                        val_metrics
                    )
            
            # LR schedule
            self.scheduler.step()
            
            print()
    
    def _save_checkpoint(self, epoch, checkpoint_dir, train_metrics, val_metrics):
        """Save model checkpoint."""
        checkpoint = {
            'epoch': epoch,
            'model_state': self.model.state_dict(),
            'optimizer_state': self.optimizer.state_dict(),
            'train_metrics': train_metrics,
            'val_metrics': val_metrics
        }
        
        path = os.path.join(checkpoint_dir, f"model_epoch_{epoch:02d}.pt")
        torch.save(checkpoint, path)
        print(f"  ğŸ’¾ Checkpoint saved: {path}")

def main():
    parser = argparse.ArgumentParser(description="Phase 4 Transformer Training")
    parser.add_argument("--data", type=str, default="data/training_data.npz", help="Training data path")
    parser.add_argument("--batch-size", type=int, default=8, help="Batch size")
    parser.add_argument("--epochs", type=int, default=10, help="Number of epochs")
    parser.add_argument("--synergy-pairs", type=str, default="data/synergy_pairs_v1.json", help="Synergy pairs file")
    parser.add_argument("--checkpoint-dir", type=str, default="checkpoints/phase4", help="Checkpoint directory")
    
    args = parser.parse_args()
    
    # Load data
    if not os.path.exists(args.data):
        print(f"âŒ Training data not found: {args.data}")
        print("   Run: python generate_transformer_training_data.py")
        return
    
    print(f"ğŸ“‚ Loading data from {args.data}...")
    data = np.load(args.data)
    
    tokens_list = [torch.from_numpy(data['tokens'][i]) for i in range(len(data['tokens']))]
    policies = torch.from_numpy(data['policies'])
    values = torch.from_numpy(data['values'])
    
    # Train/Val split
    num_train = int(0.8 * len(tokens_list))
    
    train_tokens = tokens_list[:num_train]
    train_policies = policies[:num_train]
    train_values = values[:num_train]
    
    val_tokens = tokens_list[num_train:]
    val_policies = policies[num_train:]
    val_values = values[num_train:]
    
    train_dataset = DuelDataset(
        tokens=train_tokens,
        states=None,
        policies=train_policies,
        values=train_values
    )
    
    val_dataset = DuelDataset(
        tokens=val_tokens,
        states=None,
        policies=val_policies,
        values=val_values
    )
    
    train_loader = DataLoader(
        train_dataset,
        batch_size=args.batch_size,
        shuffle=True,
        collate_fn=collate_batch
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=args.batch_size,
        collate_fn=collate_batch
    )
    
    # Train
    trainer = TransformerTrainer(synergy_pairs_path=args.synergy_pairs)
    trainer.train(train_loader, val_loader, epochs=args.epochs, checkpoint_dir=args.checkpoint_dir)

if __name__ == "__main__":
    main()
```

**å®Ÿè¡Œä¾‹**:
```bash
# Step 1: ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
python generate_transformer_training_data.py --num-samples 1000

# Step 2: å­¦ç¿’é–‹å§‹ï¼ˆãƒãƒƒãƒã‚µã‚¤ã‚º 8ï¼‰
python train_transformer_phase4.py --batch-size 8 --epochs 10

# Step 3: ãƒãƒƒãƒã‚µã‚¤ã‚ºæ®µéšçš„æ‹¡å¤§
python train_transformer_phase4.py --batch-size 16 --epochs 5
python train_transformer_phase4.py --batch-size 32 --epochs 5
python train_transformer_phase4.py --batch-size 64 --epochs 5
```

**ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ**:
- [ ] train_transformer_phase4.py å®Ÿè£…
- [ ] TransformerTrainer ã‚¯ãƒ©ã‚¹ âœ… å‹•ä½œç¢ºèª
- [ ] ãƒãƒƒãƒã‚µã‚¤ã‚º 8 ã§ 1 epoch âœ… å®Œäº†
- [ ] Loss æ›²ç·šãŒä½ä¸‹ã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèª

---

## Task 4: ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¤œè¨¼ã¨è¨˜éŒ²

**æ‰€è¦æ™‚é–“**: 0.5æ™‚é–“  
**æ‹…å½“**: ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ãƒ•ã‚§ãƒ¼ã‚º

### 4.1 ãƒãƒƒãƒã‚µã‚¤ã‚ºæ®µéšçš„æ‹¡å¤§ãƒ†ã‚¹ãƒˆ

**ã‚¹ã‚¯ãƒªãƒ—ãƒˆ**: [test_batch_scaling.py](../../test_batch_scaling.py)

```bash
# å„ãƒãƒƒãƒã‚µã‚¤ã‚ºã§ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã¨é€Ÿåº¦ã‚’æ¸¬å®š
for batch_size in 8 16 32 64; do
    echo "Testing batch size $batch_size..."
    python train_transformer_phase4.py \
        --batch-size $batch_size \
        --epochs 1 \
        --checkpoint-dir "checkpoints/batch_test_$batch_size"
done
```

**æœŸå¾…ã•ã‚Œã‚‹çµæœ**:
- ãƒãƒƒãƒã‚µã‚¤ã‚º 8: ãƒ¡ãƒ¢ãƒª ~2GB, é€Ÿåº¦ ~50 samples/sec
- ãƒãƒƒãƒã‚µã‚¤ã‚º 16: ãƒ¡ãƒ¢ãƒª ~3.5GB, é€Ÿåº¦ ~80 samples/sec
- ãƒãƒƒãƒã‚µã‚¤ã‚º 32: ãƒ¡ãƒ¢ãƒª ~6GB, é€Ÿåº¦ ~120 samples/sec
- ãƒãƒƒãƒã‚µã‚¤ã‚º 64: ãƒ¡ãƒ¢ãƒª ~10GB (RTX 3090 ã§ã¯ OOM ã®å¯èƒ½æ€§)

**ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ**:
- [ ] å„ãƒãƒƒãƒã‚µã‚¤ã‚ºã§æ­£å¸¸å‹•ä½œç¢ºèª
- [ ] ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’è¨˜éŒ²
- [ ] æœ€é©ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’æ±ºå®šï¼ˆæ¨å¥¨: 32ï¼‰

---

## å®Ÿè£…ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«

```
2026å¹´1æœˆ13æ—¥ï¼ˆWeek 2 Day 1ï¼‰

10:00-12:30 : Task 1 (2.5h)
  âœ“ synergy_pairs_v1.json ä½œæˆ
  âœ“ SynergyGraph.from_manual_pairs() å®Ÿè£…
  âœ“ test_synergy_manual.py âœ…

12:30-13:00 : æ˜¼é£Ÿ

13:00-16:00 : Task 2 (3.0h)
  âœ“ generate_transformer_training_data.py å®Ÿè£…
  âœ“ åˆæœŸ 100 ã‚µãƒ³ãƒ—ãƒ«ç”Ÿæˆ
  âœ“ test_training_data_load.py âœ…

16:00-18:30 : Task 3 (2.5h)
  âœ“ train_transformer_phase4.py å®Ÿè£…
  âœ“ TransformerTrainer ã‚¯ãƒ©ã‚¹
  âœ“ ãƒãƒƒãƒã‚µã‚¤ã‚º 8 ã§ 1 epoch âœ…

18:30-19:00 : Task 4 (0.5h)
  âœ“ ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°æ¤œè¨¼
  âœ“ ãƒ­ã‚°è¨˜éŒ²

åˆè¨ˆ: 8æ™‚é–“
```

---

## Q5-Q9 ã®æ±ºå®šåŸºæº–ï¼ˆå®Ÿè£…ä¸­ã«ç¢ºèªï¼‰

| è³ªå• | Week 2 Day 1 ã§ã®æ±ºå®šã‚¿ã‚¤ãƒŸãƒ³ã‚° | æ¨å¥¨å€¤ |
|------|------|------|
| Q5: Pos Encoding | Model initialization æ™‚ | å­¦ç¿’å¯èƒ½ï¼ˆç¾è¡Œï¼‰ |
| Q6: ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µ | Dataset å®Ÿè£…æ™‚ | Phase 2 å»¶æœŸ |
| Q7: è©•ä¾¡æŒ‡æ¨™ | Trainer metrics å®Ÿè£…æ™‚ | vs Random + vs MLP |
| Q8: ãƒ‡ãƒ—ãƒ­ã‚¤åŸºæº– | Validation metrics ç¢ºå®šæ™‚ | vs MLP â‰¥ 55% |
| Q9: Synergy Matrix | Task 1 ã§ç¢ºå®š | å¯†è¡Œåˆ—OKï¼ˆ4MBï¼‰ |

---

## Week 2 Day 1 ã®æœ€çµ‚ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

### Phase 1: ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
- [ ] GPU ç’°å¢ƒç¢ºèªï¼ˆcuda_available=Trueï¼‰
- [ ] ãƒ¡ãƒ¢ãƒªç¢ºèªï¼ˆâ‰¥ 12GB æ¨å¥¨ï¼‰
- [ ] PyTorch 2.0+ ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ç¢ºèª

### Phase 2: ãƒ‡ãƒ¼ã‚¿æº–å‚™
- [ ] synergy_pairs_v1.json ä½œæˆ âœ…
- [ ] SynergyGraph å®Ÿè£… âœ…
- [ ] Training data 1000 ã‚µãƒ³ãƒ—ãƒ«ç”Ÿæˆ âœ…

### Phase 3: ãƒ¢ãƒ‡ãƒ«ãƒ»å­¦ç¿’
- [ ] DuelTransformer åˆæœŸåŒ– âœ…
- [ ] TransformerTrainer ã‚¯ãƒ©ã‚¹ âœ…
- [ ] å­¦ç¿’ãƒ«ãƒ¼ãƒ— 1 epoch å®Œäº† âœ…

### Phase 4: æ¤œè¨¼
- [ ] Loss æ›²ç·šã‚°ãƒ©ãƒ•ä½œæˆ
- [ ] ãƒãƒƒãƒã‚µã‚¤ã‚ºæ®µéšçš„æ‹¡å¤§ãƒ†ã‚¹ãƒˆ âœ…
- [ ] ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡è¨˜éŒ²

### æœ€çµ‚ç¢ºèª
- [ ] ã™ã¹ã¦ã®ãƒ†ã‚¹ãƒˆ âœ… é€šé
- [ ] ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜ç¢ºèª
- [ ] ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆ

**Week 2 Day 1 å®Œäº†æ™‚ã®äºˆæœŸã•ã‚Œã‚‹æˆæœ**:
âœ… DuelTransformer ãŒé€šå¸¸ã«è¨“ç·´ã§ãã‚‹ã“ã¨ã‚’å®Ÿè¨¼  
âœ… Policy Loss ã¨ Value Loss ãŒä½ä¸‹ã™ã‚‹ãƒˆãƒ¬ãƒ³ãƒ‰ã‚’ç¢ºèª  
âœ… ãƒãƒƒãƒã‚µã‚¤ã‚ºæœ€é©å€¤ï¼ˆæ¨å¥¨ 32ï¼‰ã‚’æ±ºå®š  
âœ… Week 2 Day 2-3 ã¸ã®å¼•ãç¶™ãæº–å‚™å®Œäº†
