# Status and Requirements Summary (要件定義書 00)

このドキュメントはプロジェクトの現在のステータス、実装済み機能、および次のステップの要件をまとめたマスタードキュメントです。

## 1. 概要 (Overview)

Duel Masters AI Simulatorは、C++による高速なゲームエンジンと、Python/PyTorchによるAlphaZeroベースのAI学習環境を統合したプロジェクトです。
現在、コアエンジンの実装フェーズ（Phase 0: Foundation）を完了し、AIの知能進化と自己学習エコシステムの構築（Phase 1-4）へと移行しています。

## 2. 現行システムステータス (Phase 0: Foundation)

### 2.1 コアエンジン (C++)
*   **基本ルール**: マナチャージ、召喚、攻撃、ブロック、シールドブレイク、勝利条件などの基本ゲームループを実装済み。
*   **高度なメカニクス**:
    *   **スタックシステム**: トリガー能力の待機と解決順序の制御。
    *   **多色・進化**: 多色カードのマナ支払い（バックトラック法）、進化クリーチャー、NEOクリーチャー、メテオバーン。
    *   **アクションシステム**: `GenericCardSystem` と `IActionHandler` によるモジュラーな効果処理。
    *   **効果実装**: サーチ、シールド追加、除去、ハンデス、革命チェンジ、ジャストダイバー、ハイパーエナジーなど。

### 2.2 AI & 学習基盤 (Python/C++)
*   **MCTS**: `dm::ai::MCTS` によるモンテカルロ木探索と並列実行 (`ParallelRunner`)。
*   **AlphaZero**: ResNetベースのニューラルネットワークと自己対戦データ収集パイプライン。
*   **PBT準備**: 複数のエージェントを戦わせる基盤の初期実装。

### 2.3 GUI & ツール (Python/PyQt6)
*   **カードエディタ Ver 2.0**: JSONデータの視覚的な作成・編集、ロジックツリー表示、変数リンク機能。
*   **シミュレーション**: 対戦の観戦、デバッグ機能、シナリオ実行。

※ 完了した詳細な実装タスク（Phase 6-8等）は `docs/00_Overview/99_Completed_Tasks_Archive.md` にアーカイブされています。

---

## 3. デュエル・マスターズAI 総合実装計画 (Phase 1-4)

これまでの議論（不完全情報への対応、リーサル判定、自己進化システム）をすべて統合した、デュエル・マスターズ特化型AIの**総合実装計画**を以下にまとめます。
この計画は、現在のC++コア（Phase 0）を基盤とし、**「負けない戦い方（Phase 1）」から「人間的な読み（Phase 2）」**、そして**「無限の成長（Phase 3）」**へと段階的に進化させるロードマップです。

### Phase 1: 確実性の確保（ロジック強化フェーズ）

**目標**: 「詰み」の見逃しをゼロにし、不用意なトリガー踏みを防ぐ。計算コストの低い「ルールベース」でMCTSを補完する。

1.  **堅牢なリーサルソルバー (Robust Lethal Solver) の実装**
    *   **概要**: 探索の前に「勝てるか」を計算で判定するモジュール。
    *   **実装内容**:
        *   `LethalSolver` クラスをC++で作成。
        *   単純な数合わせだけでなく、`CardDefinition` を参照して以下の要素を考慮する：
            *   **攻撃能力**: スピードアタッカー (SA)、マッハファイター (MF)、進化クリーチャー。
            *   **防御能力**: ブロッカー、G・ストライク（確率的考慮または無視）。
            *   **回避能力**: ブロックされない、アンタッチャブル。
        *   毎ターン評価を行い、リーサル確定時はそのルートの評価値を最大化し、MCTS探索をバイパスまたは誘導する。

2.  **分散ベースのリスク管理評価 (Variance-Based Risk Evaluation)**
    *   **概要**: シールド・トリガーによる逆転負けを「分散（リスク）」として評価関数に組み込み、「勝率」と「安定性」のバランスを取る。
    *   **実装内容**:
        *   MCTSのバックプロパゲーションを改良し、ノードごとに勝率 ($Q$) に加えて、結果の分散（バラつき、$\sigma$）を記録する。
        *   評価式: $\text{Score} = Q - \alpha \times \sigma$
            *   $\alpha$ (アルファ): リスク回避係数。
        *   **効果**:
            *   「勝率が高いが即死リスクもある手」は、リスク項によってスコアが抑制される。
            *   「リスクはあるが、それを補って余りあるほど勝率が高い手」は採用される（過度な保守性を防ぐ）。
        *   リーサルがないときに無意味にシールドをブレイクする行為を抑制し、攻撃キャンセル（手札に戻す等）の戦略的撤退を学習可能にする。

### Phase 2: 不完全情報の克服（PIMC実装フェーズ）

**目標**: 相手の手札・シールドが見えない前提で、人間のような「読み」と「ブラフ」への対応を実現する。

1.  **自己更新型推論システム (Self-Updating Inference System) の構築**
    *   **概要**: 公開情報から相手のデッキタイプと残りのカードを推定する。
    *   **実装内容**:
        *   `meta_decks.json`: 主要メタデッキの定義ファイル。
        *   **推論ロジック**: 相手のマナ・墓地の公開情報と `meta_decks.json` をマッチングさせ、デッキタイプ確率分布を算出する。
        *   **自動学習**: 自己対戦（Phase 3）のプロセスにおいて、AIが遭遇したデッキタイプやカード構成を学習し、`meta_decks.json` を自動的に更新・拡張する仕組みを導入。人間の介入なしに新環境に適応させる。

2.  **PIMC (Perfect Information Monte Carlo) の統合**
    *   **概要**: 推論に基づき、見えない領域をランダムに埋めた「仮想世界」を複数生成して探索する。
    *   **実装内容**:
        *   探索開始時に推論結果に基づいて $N$ 個の `GameState`（世界）をサンプリング生成。
        *   `ParallelRunner` を拡張し、各世界を並列スレッドで独立してMCTS探索。
        *   **MaxMinStrategy / Vote**: 全世界の探索結果を集計（Aggregate）し、最も「最悪ケースでも機能する」あるいは「平均的に最も良い」手を選択する。

### Phase 3: 自己進化エコシステムの構築（自動学習フェーズ）

**目標**: 人間が管理せずとも、AIが勝手に自己対戦を繰り返し、環境に合わせて進化し続けるループを確立する。

1.  **AlphaZeroサイクル + PBT (Population Based Training)**
    *   **概要**: 「放置可能な進化システム」の完成。
    *   **実装内容**:
        *   **自動ループ**: `RunAlphaZeroCycle.py` (仮) により以下のフローを完全自動化。
            *   `Collect` (自己対戦: 現行最強モデル vs 過去モデル/PBT集団)
            *   `Train` (学習: 収集データを用いたモデル更新)
            *   `Evaluate` (評価: Gatekeeperによる新旧対決と入れ替え判定)
        *   **リーグ戦 (PBT)**:
            *   単一のモデルではなく、複数の異なるハイパーパラメータ（攻撃重視、守備重視、リスク係数 $\alpha$ の違いなど）を持つエージェント群（Population）を維持。
            *   エージェント同士をリーグ戦形式で競わせ、優秀な個体のパラメータを継承・変異させて次世代を作る。
    *   **期待効果**: 寝ている間にPCを稼働させるだけで、AIがメタゲームの偏り（アグロ偏重など）を自律的に修正し、多様な戦術を獲得する。

### Phase 4: アーキテクチャの刷新（将来的な最適化）

**目標**: さらなる強さと計算効率の追求。

1.  **Linear Attention / DeepSets の導入**
    *   現在のCNN/ResNetベース（固定長入力）から、可変長のカードリストを効率的に扱えるアーキテクチャへ移行。
    *   カード枚数が増えても計算速度が落ちない軽量モデルにより、推論速度（NPS: Nodes Per Second）を向上させ、探索の深さを確保する。
