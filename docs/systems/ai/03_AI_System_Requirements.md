# AIシステム要件定義書 (Requirement Definition 03)

## 1. 概要と目的
本ドキュメントは、カードゲームシミュレーションシステムにおけるAIシステムの現状の仕様、データ構造、使用アルゴリズム、およびその目的を定義するものである。
本AIシステムの主目的は以下の通りである：
1.  **対戦における勝利**: 複雑な盤面における最適手の探索と、不確定要素（S・トリガー等）のリスク管理。
2.  **自己進化 (Smart Evolution)**: 自己対戦を通じたプレイングの強化およびデッキ構築の最適化支援。
3.  **シナリオ攻略**: 特定の盤面（詰めデュエル等）における正解手の導出。

## 2. システムアーキテクチャ
本システムは、**MCTS (モンテカルロ木探索)** を中核とし、局面評価に **ニューラルネットワーク (AlphaZero方式)** または **ヒューリスティック探索 (Beam Search)** を用いるハイブリッド構成を採用している。

### コンポーネント構成
*   **Agent (意思決定層)**: MCTS, Heuristic, Random等の戦略を統括。
*   **Evaluator (評価層)**: 盤面の価値（勝率）とポリシー（次手の確率分布）を出力。
    *   `NeuralEvaluator`: 深層学習モデルによる推論。
    *   `BeamSearchEvaluator`: 短期的な探索とヒューリスティックによる評価。
*   **Inference (推論層)**: `DeckInference` および `PimcGenerator` による不完全情報推論エンジン。
*   **Solver (解析層)**: 必勝手順を探索する特化型ソルバー (`LethalSolver`)。

## 3. データ構造

### 3.1 状態表現 (State Representation)
ゲームの状態 (`GameState`) は、ニューラルネットワークへの入力として以下の**フラットなTensor (ベクトル)** に変換される。

*   **入力サイズ**: 226次元 (固定 / MLP用)
    *   **グローバル特徴 (10次元)**: ターン数、現在のフェーズ、アクティブプレイヤーID等。
    *   **プレイヤー特徴 (108次元 × 2)**: 自分と相手の各ゾーン（マナ、手札、盤面、シールド、墓地）の状態。
        *   各カードはIDやメタデータ（コスト、パワー、文明）としてエンコードされるのではなく、存在や特定の属性フラグとして抽象化される場合がある。

### 3.2 行動空間 (Action Space)
可能な全ての行動は、固定長のベクトルインデックスにマッピングされる。

*   **出力サイズ**: 約 600次元 (固定)
    *   マナチャージ: 20スロット
    *   カードプレイ: 20スロット
    *   攻撃 (クリーチャー × 対象): 約420スロット
    *   ブロック: 20スロット
    *   対象選択 (効果解決時): 100スロット
    *   パス: 1スロット
    *   その他バッファ: 10スロット

### 3.3 探索ノード (MCTS Node)
探索木を構成するノードは以下の情報を保持する。
*   `prior`: ニューラルネットワークが出力した事前確率 (P)。
*   `visit_count`: 訪問回数 (N)。
*   `value_sum`: 累積価値 (W)。
*   `children`: 子ノードへのポインタ（行動IDをキーとする）。

## 4. ロジックとアルゴリズム

### 4.1 MCTS (Monte Carlo Tree Search)
AlphaZeroアルゴリズムに準拠した探索を行う。
*   **選択 (Selection)**: PUCT (Polynomial Upper Confidence Trees) アルゴリズムを用いて、探索と活用のバランスを取る。
*   **展開 (Expansion)**: 未到達の局面でニューラルネットワーク（またはEvaluator）を呼び出し、価値(V)とポリシー(P)を取得する。
*   **バックプロパゲーション (Backpropagation)**: 葉ノードの価値をルートまで伝播させる。
    *   **Risk-Aware**: 相手のS・トリガー等による逆転リスクをペナルティとして加味するロジックが含まれる。

### 4.2 AlphaZero Network (Current)
*   **構造**: 5層 全結合層 (Fully Connected Layers, MLP)。
    *   各層 1024 ユニット、ReLU活性化関数。
*   **Head**:
    *   **Policy Head**: 行動確率を出力 (Softmax)。
    *   **Value Head**: 勝率予測を出力 (Tanh, Range [-1, 1])。

### 4.3 Lethal Solver (リーサルソルバー)
終盤において、不確定要素（S・トリガー）を考慮しつつ、確定的な勝利（ダイレクトアタック成功）が可能かを判定する。
*   **現状**: ヒューリスティックベースの判定（攻撃数 vs ブロッカー+シールド）。
*   **Future**: DFS (深さ優先探索) ベースの完全探索への移行を予定。

### 4.4 Beam Search Evaluator
MCTSのロールアウト（ランダムプレイ）の代わりに、浅い探索を行って局面の静的評価値を算出する。
*   **ビーム幅**: 7 (デフォルト)
*   **評価関数**: リソース優位性、盤面アドバンテージ、Trigger Risk。

## 5. 学習・検証パイプライン

### 5.1 データ収集 (Data Collection)
*   C++側の `DataCollector` および `ParallelRunner` により、MCTSを用いた自己対戦を並列実行。
*   生成データ: `(StateTensor, PolicyVector, Value)` のタプル。

### 5.2 学習 (Training)
*   Python (`train_simple.py`) にてPyTorchを使用。

### 5.3 検証 (Verification)
*   `ParallelRunner` を用いて、新モデル vs 旧モデル、あるいは新モデル vs ヒューリスティックの対戦を行う。

## 6. 今後の拡張性 (Future Scope)
*   **Transformerモデル (Phase 4)**: `TensorConverter` (C++) は既にトークン列への変換 (`convert_to_sequence`) をサポートしており、Python側でのモデル実装待ちである。
*   **不完全情報対応 (Phase 2)**: C++で実装された `DeckInference` をPython学習ループに統合する。
