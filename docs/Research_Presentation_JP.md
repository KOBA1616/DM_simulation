# CPU環境における大規模TCGシミュレーションのための軽量AIと探索制御手法の解明
## ―― 推論の効率化・次元圧縮・パラメータ設定法の同定 ――

---

## 1. 緒言 (Introduction)

### 研究の目的
本研究は、計算資源が制約された**CPU環境**においても、トレーディングカードゲーム（TCG）のような複雑な不完全情報ゲームの**大量かつ高負荷なシミュレーション**を実用化することを目的とする。
特に、GPUリソースに依存せずに意思決定品質を維持するため、以下の3点を解明・構築する。

1. **推論方法**: 探索空間の爆発を抑制し、計算コストを最小化する意思決定アルゴリズム。
2. **次元圧縮**: 膨大な盤面情報を効率的に処理するための、入力特徴量の圧縮手法。
3. **パラメータ設定法**: 計算時間とシミュレーション精度のトレードオフを最適化する具体的な設定値の同定。

---

## 2. 背景 (Background)

### TCGにおける課題：探索爆発
デュエル・マスターズのようなTCGは、手札や山札などの「不完全情報」と、カード効果の組み合わせによる「膨大な分岐数」を特徴とする。
- **計算量的制約**: 既存の深層強化学習（AlphaZero等）は高い性能を持つが、推論に大規模なGPU資源を必要とし、CPU環境ではレイテンシが増大して実用的な回数のシミュレーションが困難である。
- **次元の呪い**: カードの種類や配置の組み合わせが無限に近く、単純な固定長ベクトル表現では入力次元が発散し、学習・推論の効率を低下させる。

### アプローチ
「探索を強くする前に、まず探索をCPU上で成立させる」ことを基本方針とし、高効率なC++ルールエンジンと、Attention機構による動的な探索幅の制御（枝刈り）を組み合わせたハイブリッド構成を採用した。

---

## 3. 結果 (Results)

### 3.1 ハイブリッド・シミュレーション基盤の構築
- **システム構成**: C++20を用いた高速ルールエンジン（Core）と、Pythonの学習・評価モジュール（Tools）をメモリ共有型で統合。
- **データ駆動**: カード効果をJSON形式で定義し、Action/Conditionツリーとして処理することで、プログラムの変更なしに複雑なカードロジックに対応。

### 3.2 次元圧縮と軽量Transformerの実装
- **特徴量の圧縮**: 盤面情報を単純な疎行列から可変長のトークン列へと再定義し、**Transformer Encoder** を用いて文脈（Context）を効率的に圧縮する手法を確立した。
- **Synergy Biasによる探索制御**: Attention機構に対し、カード間の相互作用（コンボ）を示す **Synergy Bias** を注入。これにより、探索初期から有望な候補（Top-$k$）への重み付けが可能となり、CPU環境における探索幅の絞り込み（枝刈り）精度を大幅に向上させた。

### 3.3 CPU運用におけるパラメータ同定
実験を通じて、CPU環境下で推論レイテンシと探索性能のバランスが取れる**再現可能なパラメータ設定**を特定した（既定値として実装済み）。

| 設定項目 | 値 (Value) | 根拠・役割 |
| :--- | :--- | :--- |
| **MCTS Simulation** | 25回/手 (学習時) | 探索深さよりも試行回数を重視し、高速に大局観を学習 |
| **Search Noise** | Dirichlet $\alpha=0.3$ | 局所解への収束を防ぎ、探索の幅を確保 |
| **Batch Size** | 32 (学習), 8 (推論) | CPUキャッシュ効率と並列性の均衡点 |
| **Validation Sims** | 800回/手 | 性能検証時に必要な、統計的信頼性のある探索深さ |

---

## 4. 考察 (Discussion)

### Attentionによる資源配分の最適化
本システムのAttention機構は、単なる勝率予測器としてだけでなく、限られた**計算資源（Node Budget）をどの局面に配分するかを決定する「資源配分器」**として機能している。
CPU環境では探索の「深さ」を無制限に確保できないため、Attentionによる重要度推定で探索の「幅」を動的に制御することが、次元圧縮と高速化の鍵となる。

### 結論
- **CPU環境での成立**: 入力次元の圧縮と軽量モデルの採用により、GPU非搭載環境でも大量の自己対戦データの収集・学習サイクルを回せることが確認された。
- **パラメータの指針**: MCTSシミュレーション数を数十回オーダー（学習時）に絞り学習させ、推論時に数百回オーダーで検証する手法が、リソース対効果において有効であることを示した。

### Transformerアーキテクチャの有効性
単純なMLP（多層パーセプトロン）からTransformer Encoderへの移行により、可変長系列の処理能力と、カード間の相互作用（Synergy）の学習能力が飛躍的に向上した。これにより、CPU環境の限られた計算量内でも、より人間らしい「コンボ」や「文脈」を考慮した高度な推論が可能となったことは、本研究の重要な成果である。
